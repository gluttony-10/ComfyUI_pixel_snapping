import cv2
import numpy as np
import torch


class PixelSnappingNode:
    """
    ä½¿ç”¨SIFTç‰¹å¾åŒ¹é…å’Œä»¿å°„å˜æ¢å¯¹é½ä¸¤å¼ å›¾ç‰‡ï¼Œå¹¶æ‹¼æ¥æˆå…¨æ™¯å›¾
    é‡å åŒºåŸŸåªä¿ç•™ä¸€æ¬¡ï¼Œè¾“å‡ºåŒ…å«ä¸¤å›¾å®Œæ•´å†…å®¹çš„é•¿æ–¹å½¢
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "reference_image": ("IMAGE",),  # å‚è€ƒå›¾ï¼ˆå›¾1ï¼‰
                "target_image": ("IMAGE",),     # å¾…å¯¹é½å›¾ï¼ˆå›¾2ï¼‰
                "max_features": ("INT", {
                    "default": 5000,
                    "min": 100,
                    "max": 20000,
                    "step": 100,
                    "display": "number"
                }),
                "match_ratio": ("FLOAT", {
                    "default": 0.75,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "number"
                }),
                "ransac_threshold": ("FLOAT", {
                    "default": 5.0,
                    "min": 1.0,
                    "max": 50.0,
                    "step": 0.5,
                    "display": "number"
                }),
                "invert_input_mask": ("BOOLEAN", {
                    "default": False,
                    "label_on": "Inverted",
                    "label_off": "Normal"
                }),
                "mask_grow": ("INT", {
                    "default": 0,
                    "min": -50,
                    "max": 50,
                    "step": 1,
                    "display": "number"
                }),
                "mask_blur": ("FLOAT", {
                    "default": 0.0,
                    "min": 0.0,
                    "max": 50.0,
                    "step": 0.5,
                    "display": "number"
                }),
                "invert_output_mask": ("BOOLEAN", {
                    "default": False,
                    "label_on": "Inverted",
                    "label_off": "Normal"
                }),
            },
            "optional": {
                "target_mask": ("MASK",),  # å›¾2çš„é®ç½©ï¼ˆå¯é€‰ï¼‰
            }
        }
    
    RETURN_TYPES = ("IMAGE", "MASK", "IMAGE")
    RETURN_NAMES = ("stitched_image", "mask", "corrected_target")
    FUNCTION = "align_pixels"
    CATEGORY = "image/transform"
    
    def align_pixels(self, reference_image, target_image, max_features, match_ratio, ransac_threshold, 
                     invert_input_mask, mask_grow, mask_blur, invert_output_mask, target_mask=None):
        """
        ä½¿ç”¨SIFTç‰¹å¾åŒ¹é…å’Œä»¿å°„å˜æ¢å¯¹é½å›¾ç‰‡ï¼Œæ‹¼æ¥æˆå…¨æ™¯å›¾
        
        Args:
            reference_image: å‚è€ƒå›¾ï¼ˆå›¾1ï¼‰ï¼ŒComfyUIæ ¼å¼ [B, H, W, C]
            target_image: å¾…å¯¹é½å›¾ï¼ˆå›¾2ï¼‰ï¼ŒComfyUIæ ¼å¼ [B, H, W, C]
            max_features: SIFTç‰¹å¾ç‚¹æœ€å¤§æ•°é‡
            match_ratio: Lowe's ratio testé˜ˆå€¼
            ransac_threshold: RANSACç®—æ³•çš„åƒç´ è¯¯å·®é˜ˆå€¼
            invert_input_mask: æ˜¯å¦åè½¬è¾“å…¥é®ç½©
            mask_grow: é®ç½©æ‰©å¼ /æ”¶ç¼©åƒç´ æ•°ï¼ˆæ­£æ•°æ‰©å¼ ï¼Œè´Ÿæ•°æ”¶ç¼©ï¼‰
            mask_blur: é®ç½©æ¨¡ç³ŠåŠå¾„
            invert_output_mask: æ˜¯å¦åè½¬è¾“å‡ºé®ç½©
            target_mask: å›¾2çš„é®ç½©ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºåªè¦†ç›–ç‰¹å®šåŒºåŸŸ
        
        Returns:
            æ‹¼æ¥åçš„å…¨æ™¯å›¾ï¼ˆå›¾1+å›¾2ï¼Œé‡å åŒºåŸŸåªä¿ç•™ä¸€æ¬¡ï¼‰
        """
        # åªå¤„ç†batchä¸­çš„ç¬¬ä¸€å¼ å›¾
        ref_img = reference_image[0].cpu().numpy()  # [H, W, C]
        tgt_img = target_image[0].cpu().numpy()     # [H, W, C]
        
        # ComfyUIå›¾åƒæ ¼å¼æ˜¯ [0, 1] èŒƒå›´çš„float32ï¼Œè½¬æ¢ä¸º [0, 255] çš„uint8
        ref_img_uint8 = (ref_img * 255).astype(np.uint8)
        tgt_img_uint8 = (tgt_img * 255).astype(np.uint8)
        
        # è½¬æ¢ä¸ºç°åº¦å›¾ç”¨äºç‰¹å¾æ£€æµ‹
        if ref_img_uint8.shape[2] == 3:
            ref_gray = cv2.cvtColor(ref_img_uint8, cv2.COLOR_RGB2GRAY)
            tgt_gray = cv2.cvtColor(tgt_img_uint8, cv2.COLOR_RGB2GRAY)
        else:
            ref_gray = ref_img_uint8[:, :, 0]
            tgt_gray = tgt_img_uint8[:, :, 0]
        
        # åˆ›å»ºSIFTæ£€æµ‹å™¨
        sift = cv2.SIFT_create(nfeatures=max_features)  # type: ignore
        
        # æ£€æµ‹ç‰¹å¾ç‚¹å’Œè®¡ç®—æè¿°ç¬¦
        kp1, des1 = sift.detectAndCompute(ref_gray, None)
        kp2, des2 = sift.detectAndCompute(tgt_gray, None)
        
        if des1 is None or des2 is None or len(kp1) < 3 or len(kp2) < 3:
            print("è­¦å‘Š: ç‰¹å¾ç‚¹æ•°é‡ä¸è¶³ï¼Œè¿”å›åŸå›¾")
            return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
        
        # ä½¿ç”¨FLANNåŒ¹é…å™¨è¿›è¡Œç‰¹å¾åŒ¹é…
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)  # type: ignore
        
        # ä¿å­˜åŸå§‹å›¾2ç”¨äºé‡è¯•
        tgt_img_uint8_original = tgt_img_uint8.copy()
        tgt_gray_original = tgt_gray.copy()
        
        # å¤–å±‚å¤§å¾ªç¯ï¼šæœ€å¤šå°è¯•10æ¬¡å®Œæ•´æµç¨‹
        max_main_attempts = 10
        min_required_matches = 80
        good_matches = []  # åˆå§‹åŒ–
        
        affine_matrix = None
        ransac_mask = None
        src_pts = None
        dst_pts = None
        
        for main_attempt in range(max_main_attempts):
            print(f"\n{'='*60}")
            print(f"å°è¯• {main_attempt+1}/{max_main_attempts}")
            print(f"{'='*60}")
            
            # æ¯æ¬¡å¤§å¾ªç¯éƒ½ä»åŸå§‹å›¾2å¼€å§‹
            tgt_img_uint8 = tgt_img_uint8_original.copy()
            tgt_gray = tgt_gray_original.copy()
            
            # æ£€æµ‹ç‰¹å¾ç‚¹å’Œè®¡ç®—æè¿°ç¬¦
            kp2, des2 = sift.detectAndCompute(tgt_gray, None)
            
            if des2 is None or len(kp2) < 3:
                if main_attempt < max_main_attempts - 1:
                    print("âš ï¸ ç‰¹å¾ç‚¹ä¸è¶³ï¼Œé‡è¯•...")
                    continue
                else:
                    print("è­¦å‘Š: ç‰¹å¾ç‚¹æ•°é‡ä¸è¶³ï¼Œè¿”å›åŸå›¾")
                    return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
            
            matches = flann.knnMatch(des2, des1, k=2)
            
            # Lowe's ratio test ç­›é€‰å¥½çš„åŒ¹é…ç‚¹
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < match_ratio * n.distance:
                        good_matches.append(m)
            
            if len(good_matches) < 3:
                if main_attempt < max_main_attempts - 1:
                    print(f"âš ï¸ æœ‰æ•ˆåŒ¹é…ç‚¹ä¸è¶³ ({len(good_matches)}ä¸ª)ï¼Œé‡è¯•...")
                    continue
                else:
                    print(f"è­¦å‘Š: æœ‰æ•ˆåŒ¹é…ç‚¹æ•°é‡ä¸è¶³ ({len(good_matches)}ä¸ª)ï¼Œè¿”å›åŸå›¾")
                    return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
            
            print(f"æ‰¾åˆ° {len(good_matches)} ä¸ªæœ‰æ•ˆåŒ¹é…ç‚¹")
            
            # æå–åŒ¹é…ç‚¹çš„åæ ‡
            src_pts = np.float32([kp2[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # type: ignore
            dst_pts = np.float32([kp1[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # type: ignore
            
            # ä½¿ç”¨RANSACä¼°è®¡ä»¿å°„å˜æ¢çŸ©é˜µï¼ˆå®Œæ•´ä»¿å°„å˜æ¢ï¼Œæ”¯æŒéå‡åŒ€ç¼©æ”¾å’Œå‰ªåˆ‡ï¼‰
            affine_matrix, ransac_mask = cv2.estimateAffine2D(
                src_pts, 
                dst_pts, 
                method=cv2.RANSAC,
                ransacReprojThreshold=ransac_threshold
            )
            
            if affine_matrix is None:
                if main_attempt < max_main_attempts - 1:
                    print("âš ï¸ æ— æ³•è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µï¼Œé‡è¯•...")
                    continue
                else:
                    print("è­¦å‘Š: æ— æ³•è®¡ç®—ä»¿å°„å˜æ¢çŸ©é˜µï¼Œè¿”å›åŸå›¾")
                    return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
            
            inliers = np.sum(ransac_mask)
            print(f"RANSACå†…ç‚¹æ•°é‡: {inliers}/{len(good_matches)}")
            
            # ã€ä¼˜åŒ–ã€‘åŸºäºé‡æŠ•å½±è¯¯å·®è¿‡æ»¤ä½è´¨é‡åŒ¹é…ç‚¹åé‡æ–°è®¡ç®—
            inlier_mask = ransac_mask.ravel() == 1
            src_inliers = src_pts[inlier_mask]
            dst_inliers = dst_pts[inlier_mask]
            
            # è®¡ç®—æ‰€æœ‰å†…ç‚¹çš„é‡æŠ•å½±è¯¯å·®
            transformed = cv2.transform(src_inliers, affine_matrix)
            errors = np.linalg.norm(transformed - dst_inliers, axis=2).ravel()
            
            # åªä¿ç•™è¯¯å·®æœ€å°çš„80%ç‚¹
            threshold_80 = np.percentile(errors, 80)
            good_mask = errors <= threshold_80
            refined_count = np.sum(good_mask)
            
            print(f"ğŸ”§ é‡æŠ•å½±è¯¯å·®è¿‡æ»¤: ä¿ç•™{refined_count}/{inliers}ä¸ªæœ€ä¼˜å†…ç‚¹ (80%åˆ†ä½)")
            
            # ç”¨æœ€ä¼˜ç‚¹é‡æ–°è®¡ç®—ä»¿å°„çŸ©é˜µ
            if refined_count >= 3:  # è‡³å°‘éœ€è¦3ä¸ªç‚¹
                src_pts_refined = src_inliers[good_mask]
                dst_pts_refined = dst_inliers[good_mask]
                affine_matrix_refined, _ = cv2.estimateAffine2D(
                    src_pts_refined, 
                    dst_pts_refined,
                    method=cv2.LMEDS  # ä½¿ç”¨æœ€å°ä¸­å€¼æ³•ï¼Œå¯¹ç¦»ç¾¤ç‚¹æ›´é²æ£’
                )
                
                if affine_matrix_refined is not None:
                    affine_matrix = affine_matrix_refined
                    print("âœ“ å·²ä½¿ç”¨ç²¾åŒ–åçš„ä»¿å°„çŸ©é˜µ")
            
            # æ›´æ–°ransac_maskå’Œsrc_pts/dst_ptsä»¥ä¾¿åç»­ä½¿ç”¨
            if refined_count >= 3:
                # é‡å»ºå®Œæ•´çš„maskå’Œç‚¹é›†ï¼ˆç”¨äºåç»­çš„åŒ…å›´æ¡†è®¡ç®—ï¼‰
                temp_mask = np.zeros(len(good_matches), dtype=bool)
                inlier_indices = np.where(inlier_mask)[0]
                refined_indices = inlier_indices[good_mask]
                temp_mask[refined_indices] = True
                ransac_mask = temp_mask.reshape(-1, 1).astype(np.uint8)
            
            # åªä½¿ç”¨RANSACå†…ç‚¹è®¡ç®—åŒ…å›´æ¡†
            inlier_mask = ransac_mask.ravel() == 1
            src_pts_inliers = src_pts[inlier_mask].reshape(-1, 2)  # å›¾2çš„å†…ç‚¹
            dst_pts_inliers = dst_pts[inlier_mask].reshape(-1, 2)  # å›¾1çš„å†…ç‚¹
            
            # è®¡ç®—RANSACå†…ç‚¹çš„åŒ…å›´æ¡†ï¼ˆåªç”¨æœ€å¯é çš„åŒ¹é…ç‚¹ï¼‰
            src_pts_2d = src_pts_inliers  # å›¾2ä¸­çš„RANSACå†…ç‚¹
            dst_pts_2d = dst_pts_inliers  # å›¾1ä¸­çš„RANSACå†…ç‚¹
            
            # æ‰“å°å›¾ç‰‡å°ºå¯¸å’ŒåŒ¹é…ä¿¡æ¯
            h1, w1 = ref_img_uint8.shape[:2]
            h2, w2 = tgt_img_uint8.shape[:2]
            print(f"å›¾1å°ºå¯¸: {w1}Ã—{h1}")
            print(f"å›¾2å°ºå¯¸: {w2}Ã—{h2}")
            
            # å›¾1åŒ¹é…ç‚¹çš„åŒ…å›´æ¡†
            dst_x_min, dst_y_min = np.min(dst_pts_2d, axis=0)
            dst_x_max, dst_y_max = np.max(dst_pts_2d, axis=0)
            dst_bbox_width = dst_x_max - dst_x_min
            dst_bbox_height = dst_y_max - dst_y_min
            
            # å›¾2åŒ¹é…ç‚¹çš„åŒ…å›´æ¡†
            src_x_min, src_y_min = np.min(src_pts_2d, axis=0)
            src_x_max, src_y_max = np.max(src_pts_2d, axis=0)
            src_bbox_width = src_x_max - src_x_min
            src_bbox_height = src_y_max - src_y_min
            
            print(f"å›¾1åŒ…å›´æ¡†ï¼ˆåŸºäº{inliers}ä¸ªå†…ç‚¹ï¼‰: {dst_bbox_width:.1f}Ã—{dst_bbox_height:.1f}")
            print(f"å›¾2åŒ…å›´æ¡†ï¼ˆåŸºäº{inliers}ä¸ªå†…ç‚¹ï¼‰: {src_bbox_width:.1f}Ã—{src_bbox_height:.1f}")
            
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_a = dst_bbox_width / src_bbox_width if src_bbox_width > 0 else 1.0  # å®½åº¦ç¼©æ”¾æ¯”ä¾‹
            scale_b = dst_bbox_height / src_bbox_height if src_bbox_height > 0 else 1.0  # é«˜åº¦ç¼©æ”¾æ¯”ä¾‹
            
            print(f"æ•°å€¼ a (å®½åº¦æ¯”ä¾‹): {scale_a:.3f}")
            print(f"æ•°å€¼ b (é«˜åº¦æ¯”ä¾‹): {scale_b:.3f}")
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å½¢å˜ä¿®æ­£
            threshold = 0.006  # 0.6%çš„åå·®é˜ˆå€¼
            if abs(scale_a - scale_b) > threshold:
                print(f"="*60)
                print(f"âš ï¸ æ£€æµ‹åˆ°å›¾2å­˜åœ¨å½¢å˜ï¼Œaä¸bå·®å¼‚: {abs(scale_a - scale_b):.3f}")
                
                h2, w2 = tgt_img_uint8.shape[:2]
                print(f"å½“å‰å›¾2å°ºå¯¸: {w2}Ã—{h2}")
                
                if scale_a > scale_b:
                    # a > b â†’ éœ€è¦æ‹‰ä¼¸å›¾2çš„å®½åº¦
                    # ä¿®æ­£åå›¾2åŒ…å›´æ¡†çš„å®½åº¦ = å›¾1åŒ…å›´æ¡†å®½åº¦ Ã— (å›¾2åŒ…å›´æ¡†é«˜åº¦ / å›¾1åŒ…å›´æ¡†é«˜åº¦)
                    corrected_bbox_width = dst_bbox_width * (src_bbox_height / dst_bbox_height)
                    # å›¾2æ•´å›¾ä¿®æ­£åçš„å®½åº¦ = å›¾2åŸå›¾å®½åº¦ Ã— (ä¿®æ­£åå›¾2åŒ…å›´æ¡†çš„å®½åº¦ / å›¾2åŒ…å›´æ¡†çš„å®½åº¦)
                    corrected_w = int(w2 * (corrected_bbox_width / src_bbox_width))
                    corrected_h = h2
                    print(f"ä¿®æ­£ååŒ…å›´æ¡†å®½åº¦: {corrected_bbox_width:.1f} (åŸ{src_bbox_width:.1f})")
                    print(f"ğŸ”§ æ¨ªå‘æ‹‰ä¼¸ä¿®æ­£ (a > b): {w2}Ã—{h2} -> {corrected_w}Ã—{corrected_h}")
                else:
                    # a < b â†’ éœ€è¦æ‹‰ä¼¸å›¾2çš„é«˜åº¦
                    # ä¿®æ­£åå›¾2åŒ…å›´æ¡†çš„é«˜åº¦ = å›¾1åŒ…å›´æ¡†é«˜åº¦ Ã— (å›¾2åŒ…å›´æ¡†å®½åº¦ / å›¾1åŒ…å›´æ¡†å®½åº¦)
                    corrected_bbox_height = dst_bbox_height * (src_bbox_width / dst_bbox_width)
                    # å›¾2æ•´å›¾ä¿®æ­£åçš„é«˜åº¦ = å›¾2åŸå›¾é«˜åº¦ Ã— (ä¿®æ­£åå›¾2åŒ…å›´æ¡†çš„é«˜åº¦ / å›¾2åŒ…å›´æ¡†çš„é«˜åº¦)
                    corrected_w = w2
                    corrected_h = int(h2 * (corrected_bbox_height / src_bbox_height))
                    print(f"ä¿®æ­£ååŒ…å›´æ¡†é«˜åº¦: {corrected_bbox_height:.1f} (åŸ{src_bbox_height:.1f})")
                    print(f"ğŸ”§ çºµå‘æ‹‰ä¼¸ä¿®æ­£ (a < b): {w2}Ã—{h2} -> {corrected_w}Ã—{corrected_h}")
                
                print(f"="*60)
                
                # æ‰§è¡Œæ‹‰ä¼¸
                tgt_img_uint8 = cv2.resize(tgt_img_uint8, (corrected_w, corrected_h), interpolation=cv2.INTER_CUBIC)
                
                # å¦‚æœæœ‰è¾“å…¥é®ç½©ï¼ŒåŒæ­¥æ‹‰ä¼¸
                if target_mask is not None:
                    input_mask_original = target_mask[0].cpu().numpy()
                    if input_mask_original.shape[0] != h2 or input_mask_original.shape[1] != w2:
                        input_mask_original = cv2.resize(input_mask_original, (w2, h2), interpolation=cv2.INTER_LINEAR)
                    input_mask_corrected = cv2.resize(input_mask_original, (corrected_w, corrected_h), interpolation=cv2.INTER_LINEAR)
                
                # é‡æ–°è½¬æ¢ä¸ºç°åº¦å›¾å¹¶é‡æ–°åŒ¹é…
                tgt_gray = cv2.cvtColor(tgt_img_uint8, cv2.COLOR_RGB2GRAY) if tgt_img_uint8.shape[2] == 3 else tgt_img_uint8[:, :, 0]
                
                print("ğŸ” ä½¿ç”¨ä¿®æ­£åçš„å›¾2é‡æ–°è¿›è¡ŒSIFTåŒ¹é…...")
                kp2, des2 = sift.detectAndCompute(tgt_gray, None)
                
                if des2 is None or len(kp2) < 3:
                    print("âš ï¸ è­¦å‘Š: ä¿®æ­£åç‰¹å¾ç‚¹ä¸è¶³ï¼Œå›åˆ°å¤§å¾ªç¯")
                    continue
                
                matches = flann.knnMatch(des2, des1, k=2)
                
                good_matches = []
                for match_pair in matches:
                    if len(match_pair) == 2:
                        m, n = match_pair
                        if m.distance < match_ratio * n.distance:
                            good_matches.append(m)
                
                if len(good_matches) < 3:
                    print(f"âš ï¸ è­¦å‘Š: ä¿®æ­£åæœ‰æ•ˆåŒ¹é…ç‚¹ä¸è¶³ ({len(good_matches)}ä¸ª)ï¼Œå›åˆ°å¤§å¾ªç¯")
                    continue
                
                print(f"ä¿®æ­£åæ‰¾åˆ° {len(good_matches)} ä¸ªæœ‰æ•ˆåŒ¹é…ç‚¹")
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°åŒ¹é…ç‚¹è¦æ±‚
                if len(good_matches) < min_required_matches:
                    print(f"âš ï¸ ä¿®æ­£ååŒ¹é…ç‚¹æ•°é‡ {len(good_matches)} ä½äº {min_required_matches}ï¼Œå›åˆ°å¤§å¾ªç¯")
                    continue
                
                # é‡æ–°æå–åŒ¹é…ç‚¹åæ ‡ï¼ˆä½¿ç”¨ä¿®æ­£åçš„å›¾2ï¼‰
                src_pts = np.float32([kp2[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # type: ignore
                dst_pts = np.float32([kp1[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # type: ignore
                
                # é‡æ–°è®¡ç®—RANSACä»¿å°„å˜æ¢çŸ©é˜µï¼ˆå®Œæ•´ä»¿å°„å˜æ¢ï¼‰
                affine_matrix, ransac_mask = cv2.estimateAffine2D(
                    src_pts, 
                    dst_pts, 
                    method=cv2.RANSAC,
                    ransacReprojThreshold=ransac_threshold
                )
                
                if affine_matrix is None:
                    print("âš ï¸ è­¦å‘Š: ä¿®æ­£åæ— æ³•è®¡ç®—ä»¿å°„å˜æ¢ï¼Œè¿”å›åŸå›¾")
                    return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
                
                # ã€ä¼˜åŒ–ã€‘åŸºäºé‡æŠ•å½±è¯¯å·®è¿‡æ»¤ä½è´¨é‡åŒ¹é…ç‚¹åé‡æ–°è®¡ç®—
                inliers_corrected = np.sum(ransac_mask)
                print(f"ä¿®æ­£åRANSACå†…ç‚¹æ•°é‡: {inliers_corrected}/{len(good_matches)}")
                
                inlier_mask_corrected = ransac_mask.ravel() == 1
                src_inliers_corrected = src_pts[inlier_mask_corrected]
                dst_inliers_corrected = dst_pts[inlier_mask_corrected]
                
                # è®¡ç®—æ‰€æœ‰å†…ç‚¹çš„é‡æŠ•å½±è¯¯å·®
                transformed_corrected = cv2.transform(src_inliers_corrected, affine_matrix)
                errors_corrected = np.linalg.norm(transformed_corrected - dst_inliers_corrected, axis=2).ravel()
                
                # åªä¿ç•™è¯¯å·®æœ€å°çš„80%ç‚¹
                threshold_80_corrected = np.percentile(errors_corrected, 80)
                good_mask_corrected = errors_corrected <= threshold_80_corrected
                refined_count_corrected = np.sum(good_mask_corrected)
                
                print(f"ğŸ”§ ä¿®æ­£åé‡æŠ•å½±è¯¯å·®è¿‡æ»¤: ä¿ç•™{refined_count_corrected}/{inliers_corrected}ä¸ªæœ€ä¼˜å†…ç‚¹ (80%åˆ†ä½)")
                
                # ç”¨æœ€ä¼˜ç‚¹é‡æ–°è®¡ç®—ä»¿å°„çŸ©é˜µ
                if refined_count_corrected >= 3:
                    src_pts_refined_corrected = src_inliers_corrected[good_mask_corrected]
                    dst_pts_refined_corrected = dst_inliers_corrected[good_mask_corrected]
                    affine_matrix_refined_corrected, _ = cv2.estimateAffine2D(
                        src_pts_refined_corrected, 
                        dst_pts_refined_corrected,
                        method=cv2.LMEDS
                    )
                    
                    if affine_matrix_refined_corrected is not None:
                        affine_matrix = affine_matrix_refined_corrected
                        print("âœ“ å·²ä½¿ç”¨ä¿®æ­£åç²¾åŒ–çš„ä»¿å°„çŸ©é˜µ")
                        
                        # æ›´æ–°ransac_mask
                        temp_mask_corrected = np.zeros(len(good_matches), dtype=bool)
                        inlier_indices_corrected = np.where(inlier_mask_corrected)[0]
                        refined_indices_corrected = inlier_indices_corrected[good_mask_corrected]
                        temp_mask_corrected[refined_indices_corrected] = True
                        ransac_mask = temp_mask_corrected.reshape(-1, 1).astype(np.uint8)
                
                # éªŒè¯ä¿®æ­£æ•ˆæœï¼ˆåŸºäºRANSACå†…ç‚¹ï¼‰
                inlier_mask_new = ransac_mask.ravel() == 1
                src_pts_2d_new = src_pts[inlier_mask_new].reshape(-1, 2)
                dst_pts_2d_new = dst_pts[inlier_mask_new].reshape(-1, 2)
                
                src_bbox_w_new = np.max(src_pts_2d_new[:, 0]) - np.min(src_pts_2d_new[:, 0])  # type: ignore
                src_bbox_h_new = np.max(src_pts_2d_new[:, 1]) - np.min(src_pts_2d_new[:, 1])  # type: ignore
                dst_bbox_w_new = np.max(dst_pts_2d_new[:, 0]) - np.min(dst_pts_2d_new[:, 0])  # type: ignore
                dst_bbox_h_new = np.max(dst_pts_2d_new[:, 1]) - np.min(dst_pts_2d_new[:, 1])  # type: ignore
                
                scale_a_new = dst_bbox_w_new / src_bbox_w_new if src_bbox_w_new > 0 else 1.0
                scale_b_new = dst_bbox_h_new / src_bbox_h_new if src_bbox_h_new > 0 else 1.0
                
                print(f"ä¿®æ­£å: a={scale_a_new:.3f}, b={scale_b_new:.3f}, å·®å¼‚={abs(scale_a_new - scale_b_new):.3f}")
                print(f"âœ“ æ”¹å–„æ•ˆæœ: {abs(scale_a - scale_b):.3f} -> {abs(scale_a_new - scale_b_new):.3f}")
            else:
                print(f"âœ“ å›¾2æ— æ˜æ˜¾å½¢å˜ (å·®å¼‚: {abs(scale_a - scale_b):.3f})")
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°åŒ¹é…ç‚¹è¦æ±‚
            if len(good_matches) >= min_required_matches:
                print(f"âœ“ åŒ¹é…ç‚¹æ•°é‡æ»¡è¶³è¦æ±‚ ({len(good_matches)}>={min_required_matches})")
                break
            else:
                if main_attempt < max_main_attempts - 1:
                    print(f"âš ï¸ åŒ¹é…ç‚¹æ•°é‡ {len(good_matches)} ä½äº {min_required_matches}ï¼Œå›åˆ°å¾ªç¯å¼€å§‹é‡è¯•...")
                    continue
                else:
                    print(f"âš ï¸ è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼ŒåŒ¹é…ç‚¹æ•°é‡ {len(good_matches)} ä»ä½äº {min_required_matches}ï¼Œè¿”å›åŸå›¾")
                    return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
        
        # å¤§å¾ªç¯ç»“æŸåï¼Œæœ€ç»ˆæ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
        if len(good_matches) < min_required_matches:
            print(f"âš ï¸ æœ€ç»ˆæ£€æŸ¥: åŒ¹é…ç‚¹æ•°é‡ {len(good_matches)} ä½äº {min_required_matches}ï¼Œè¿”å›åŸå›¾")
            return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
        
        # ç¡®ä¿å˜é‡å·²æ­£ç¡®èµ‹å€¼
        if affine_matrix is None or ransac_mask is None or src_pts is None or dst_pts is None:
            print("è­¦å‘Š: å¤„ç†å¼‚å¸¸ï¼Œè¿”å›åŸå›¾")
            return (target_image, torch.zeros(1, target_image.shape[1], target_image.shape[2]), target_image)
        
        # ä¿å­˜ä¿®æ­£åçš„å›¾2ç”¨äºè¾“å‡º
        corrected_target_img = tgt_img_uint8.astype(np.float32) / 255.0
        corrected_target_tensor = torch.from_numpy(corrected_target_img).unsqueeze(0)
        
        # è·å–å‚è€ƒå›¾çš„å°ºå¯¸
        h1, w1 = ref_img_uint8.shape[:2]
        h2, w2 = tgt_img_uint8.shape[:2]
        
        # ä½¿ç”¨å›¾1ä½œä¸ºç”»å¸ƒå°ºå¯¸ï¼ˆç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸å›¾1ä¸€è‡´ï¼‰
        canvas_w = w1
        canvas_h = h1
        
        print(f"ç”»å¸ƒå°ºå¯¸ï¼ˆä½¿ç”¨å›¾1å°ºå¯¸ï¼‰: {canvas_w}Ã—{canvas_h}")
        
        # åˆ›å»ºç”»å¸ƒï¼Œç›´æ¥ä½¿ç”¨å›¾1ä½œä¸ºåº•å›¾
        canvas = ref_img_uint8.copy()
        
        # å¯¹å›¾2åº”ç”¨ä»¿å°„å˜æ¢åˆ°ç”»å¸ƒä¸Šï¼ˆç›´æ¥å˜æ¢åˆ°å›¾1çš„åæ ‡ç³»ï¼‰
        aligned_img_uint8 = cv2.warpAffine(
            tgt_img_uint8,
            affine_matrix,
            (canvas_w, canvas_h),
            flags=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT,
            borderValue=(0, 0, 0)
        )
        
        # åˆ›å»ºæˆ–å¤„ç†é®ç½©
        if target_mask is not None:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¿®æ­£åçš„é®ç½©ï¼ˆå¦‚æœè¿›è¡Œäº†å½¢å˜ä¿®æ­£ï¼‰
            if 'input_mask_corrected' in locals() and input_mask_corrected is not None:  # type: ignore
                input_mask = input_mask_corrected  # type: ignore
                print("ä½¿ç”¨ä¿®æ­£åçš„é®ç½©")
            else:
                # ä½¿ç”¨è¾“å…¥çš„é®ç½©
                input_mask = target_mask[0].cpu().numpy()  # [H, W]
                
                # ç¡®ä¿é®ç½©å°ºå¯¸ä¸å›¾2ä¸€è‡´
                h2, w2 = tgt_img_uint8.shape[:2]
                if input_mask.shape[0] != h2 or input_mask.shape[1] != w2:
                    print(f"è­¦å‘Š: é®ç½©å°ºå¯¸{input_mask.shape}ä¸å›¾2å°ºå¯¸{(h2, w2)}ä¸ä¸€è‡´ï¼Œè¿›è¡Œç¼©æ”¾")
                    input_mask = cv2.resize(input_mask, (w2, h2), interpolation=cv2.INTER_LINEAR)
            
            # åè½¬è¾“å…¥é®ç½©ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if invert_input_mask:
                input_mask = 1.0 - input_mask
            
            # å°†é®ç½©å˜æ¢åˆ°ç”»å¸ƒä¸Šï¼ˆä½¿ç”¨åŸå§‹affine_matrixï¼‰
            mask_2d = cv2.warpAffine(  # type: ignore
                input_mask,
                affine_matrix,
                (canvas_w, canvas_h),
                flags=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=0.0  # type: ignore
            )
        else:
            # ä¸æä¾›é®ç½©æ—¶ï¼Œä½¿ç”¨å›¾2çš„å®Œæ•´åŒºåŸŸï¼ˆæ£€æµ‹å›¾2å˜æ¢åçš„æœ‰æ•ˆè¾¹ç•Œï¼‰
            # åˆ›å»ºå›¾2åŸå§‹å°ºå¯¸çš„å…¨1é®ç½©
            h2, w2 = tgt_img_uint8.shape[:2]
            full_mask = np.ones((h2, w2), dtype=np.float32)
            
            # å°†å®Œæ•´é®ç½©å˜æ¢åˆ°ç”»å¸ƒä¸Šï¼ˆä½¿ç”¨åŸå§‹affine_matrixï¼‰
            mask_2d = cv2.warpAffine(  # type: ignore
                full_mask,
                affine_matrix,
                (canvas_w, canvas_h),
                flags=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=0.0  # type: ignore
            )
        
        # é®ç½©æ‰©å¼ /æ”¶ç¼©
        if mask_grow != 0:
            if mask_grow > 0:
                # æ‰©å¼ 
                kernel_size = mask_grow * 2 + 1
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
                mask_2d = cv2.dilate(mask_2d, kernel, iterations=1)
            else:
                # æ”¶ç¼©
                kernel_size = abs(mask_grow) * 2 + 1
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
                mask_2d = cv2.erode(mask_2d, kernel, iterations=1)
        
        # é®ç½©æ¨¡ç³Š
        if mask_blur > 0:
            kernel_size = int(mask_blur * 2) * 2 + 1  # ç¡®ä¿æ˜¯å¥‡æ•°
            mask_2d = cv2.GaussianBlur(mask_2d, (kernel_size, kernel_size), mask_blur)
        
        # ç¡®ä¿é®ç½©å€¼åŸŸåœ¨[0, 1]
        mask_2d = np.clip(mask_2d, 0.0, 1.0)
        
        mask_img = mask_2d[:, :, np.newaxis]  # [H, W, 1]
        
        # å°†å›¾2å åŠ åˆ°ç”»å¸ƒä¸Šï¼ˆä½¿ç”¨å¤„ç†åçš„é®ç½©è¿›è¡Œæ··åˆï¼‰
        final_img_uint8 = (canvas * (1 - mask_img) + aligned_img_uint8 * mask_img).astype(np.uint8)
        
        # è½¬æ¢å›ComfyUIæ ¼å¼
        final_img = final_img_uint8.astype(np.float32) / 255.0
        final_tensor = torch.from_numpy(final_img).unsqueeze(0)
        
        # è¾“å‡ºé®ç½©ï¼ˆä½¿ç”¨å¤„ç†åçš„é®ç½©ï¼‰
        output_mask = mask_2d.copy()
        
        # åè½¬è¾“å‡ºé®ç½©ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if invert_output_mask:
            output_mask = 1.0 - output_mask
        
        mask_tensor = torch.from_numpy(output_mask).unsqueeze(0)  # [1, H, W]
        
        return (final_tensor, mask_tensor, corrected_target_tensor)


# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "PixelSnapping": PixelSnappingNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "PixelSnapping": "Pixel Snapping (SIFT)"
}